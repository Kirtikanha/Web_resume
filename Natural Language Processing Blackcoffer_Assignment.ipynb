{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Function to read files\n",
    "def read_files(file_paths):\n",
    "    contents = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                contents.append(file.read())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    return contents\n",
    "file_paths = [\n",
    "    \"Stopwords/StopWords_Auditor.txt\",\n",
    "    \"Stopwords/StopWords_DatesandNumbers.txt\",\n",
    "    \"Stopwords/StopWords_Generic.txt\",\n",
    "    \"Stopwords/StopWords_GenericLong.txt\",\n",
    "    \"Stopwords/StopWords_Names.txt\"\n",
    "]\n",
    "\n",
    "# Call the function and pass the file paths\n",
    "file_contents = read_files(file_paths)\n",
    "\n",
    "# Write the contents of each file to \"stopwords_1.txt\"\n",
    "with open(\"stopwords_1.txt\", 'a', encoding='utf-8') as f:\n",
    "    for content in file_contents:\n",
    "        f.write(content)\n",
    "\n",
    "# Function to extract text from URL\n",
    "def extract_text(url):\n",
    "    with open(\"scraped_data.txt\", 'a', encoding='utf-8') as f:\n",
    "        for i in range(0, len(df)):\n",
    "        # Check if the URL is valid\n",
    "            url = df.iloc[i, 0]  # Assuming URL is in the first column of the DataFrame\n",
    "            if pd.isna(url):\n",
    "                # Skip this iteration if the URL is NaN\n",
    "                continue\n",
    "\n",
    "            # Check if the URL has a schema (e.g., 'http://' or 'https://')\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                # Add 'http://' as the default schema if it's missing\n",
    "                url = 'http://' + url\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL {url}: {e}\")\n",
    "                continue  # Skip this URL if there's an error\n",
    "\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # Extracting title\n",
    "            title_element = soup.find(attrs={'class': 'entry-title'})\n",
    "            if title_element:\n",
    "                title = title_element.text.strip()\n",
    "            else:\n",
    "                title = \"Title not found\"\n",
    "\n",
    "            # Extracting content\n",
    "            content_element = soup.find(attrs={'class': 'td-post-content'})\n",
    "            if content_element:\n",
    "                content = content_element.text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "            else:\n",
    "                content = \"Content not found\"\n",
    "\n",
    "            # Merge title and content\n",
    "            text = f\"{title}. {content}\"\n",
    "\n",
    "            # Write the text to the file\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "def positive(file_path_pos):\n",
    "    with open(file_path_pos, 'r', encoding='latin-1') as file_neg:\n",
    "        positive_words = file_neg.readlines()  # Use readlines() to get a list of lines from the file\n",
    "    # Strip any leading/trailing whitespace and newline characters from each word\n",
    "    positive_words = [word.strip() for word in positive_words]\n",
    "    return positive_words\n",
    "\n",
    "def negative(file_path_neg):\n",
    "    with open(file_path_neg, 'r', encoding='latin-1') as file_neg:\n",
    "        negative_words = file_neg.readlines()  # Use readlines() to get a list of lines from the file\n",
    "    # Strip any leading/trailing whitespace and newline characters from each word\n",
    "    negative_words = [word.strip() for word in negative_words]\n",
    "    return negative_words\n",
    "\n",
    "# Read positive and negative words\n",
    "file_path_pos = \"positive-words.txt\"\n",
    "file_path_neg = \"negative-words.txt\"\n",
    "\n",
    "positive_words = read_files(file_path_pos)\n",
    "negative_words = read_files(file_path_neg)\n",
    "   \n",
    "    \n",
    "# Function to compute textual analysis variables\n",
    "def compute_text_variables(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_list = read_files(file_paths)\n",
    "\n",
    "    tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
    "    \n",
    "    # Compute other variables\n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / max(1, positive_score + negative_score)\n",
    "    subjectivity_score = 0  # You need a more advanced method to calculate subjectivity score\n",
    "    avg_sentence_length = len(tokens) / len(sent_tokenize(text))\n",
    "    complex_words = [token for token in tokens if len(token) > 6]  # Assume words with more than 6 characters are complex\n",
    "    percentage_complex_words = (len(complex_words) / len(tokens)) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_num_words_per_sentence = len(tokens) / len(sent_tokenize(text))\n",
    "    complex_word_count = len(complex_words)\n",
    "    word_count = len(tokens)\n",
    "    syllable_per_word = 0  # You need a syllable count function to calculate this\n",
    "    personal_pronouns = sum(1 for word in tokens if word.lower() in [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\"])\n",
    "    avg_word_length = sum(len(word) for word in tokens) / len(tokens)\n",
    "    \n",
    "    return [positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
    "            percentage_complex_words, fog_index, avg_num_words_per_sentence, complex_word_count,\n",
    "            word_count, syllable_per_word, personal_pronouns, avg_word_length]\n",
    "\n",
    "\n",
    "# Load input data\n",
    "data = pd.read_csv('input.csv')\n",
    "df = data.drop('URL_ID', axis=1)\n",
    "\n",
    "# Extract text from URLs, compute variables, and save output\n",
    "output_data = []\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    title, content = extract_text(url)\n",
    "    text = f\"{title}. {content}\"\n",
    "    text_variables = compute_text_variables(text)\n",
    "    output_data.append([url_id, title] + text_variables)\n",
    "\n",
    "# Create DataFrame for output data\n",
    "output_columns = [\"URL_ID\", \"Article_Title\", \"POSITIVE_SCORE\", \"NEGATIVE_SCORE\", \"POLARITY_SCORE\", \"SUBJECTIVITY_SCORE\",\n",
    "                  \"AVG_SENTENCE_LENGTH\", \"PERCENTAGE_OF_COMPLEX_WORDS\", \"FOG_INDEX\", \"AVG_NUMBER_OF_WORDS_PER_SENTENCE\",\n",
    "                  \"COMPLEX_WORD_COUNT\", \"WORD_COUNT\", \"SYLLABLE_PER_WORD\", \"PERSONAL_PRONOUNS\", \"AVG_WORD_LENGTH\"]\n",
    "output_df = pd.DataFrame(output_data, columns=output_columns)\n",
    "\n",
    "# Save output to Excel file\n",
    "output_df.to_excel(\"TextAnalysisOutput.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defbbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
